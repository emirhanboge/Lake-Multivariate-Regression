{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23774,"status":"ok","timestamp":1688495139336,"user":{"displayName":"Emirhan Böge","userId":"05794559765456875582"},"user_tz":-180},"id":"iQyW7HPDee4l","outputId":"205002df-9f77-461c-88dc-28176f8a274e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmOywS8Jd87r"},"outputs":[],"source":["%%capture\n","!pip3 install timm transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4223,"status":"ok","timestamp":1687180406312,"user":{"displayName":"Emirhan Böge","userId":"05794559765456875582"},"user_tz":-180},"id":"XXpSBwwydoko","outputId":"98753b10-5f73-4611-92c8-11f5984c2c41"},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7f13ed736610>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import numpy as np\n","import h5py\n","import copy\n","\n","from tqdm import tqdm\n","import timm\n","\n","from transformers import AutoImageProcessor, ViTForImageClassification\n","\n","import torch\n","import torch.nn as nna\n","from torch.utils.data import Dataset, DataLoader, SequentialSampler\n","from torch import optim\n","\n","from torchvision import transforms\n","\n","import pandas as pd\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import r2_score, mean_squared_error\n","\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1687180406312,"user":{"displayName":"Emirhan Böge","userId":"05794559765456875582"},"user_tz":-180},"id":"YXccPqLhdrLN","outputId":"c0fdb94d-e92f-45b5-fbe1-e09d2404e081"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-72ecfeb8-101f-46f5-8147-5bfdaf5fa9be\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Station</th>\n","      <th>Klorofil-a (µg/L)</th>\n","      <th>X</th>\n","      <th>Y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2017-04-27</td>\n","      <td>1</td>\n","      <td>86.14</td>\n","      <td>235</td>\n","      <td>537</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2017-04-27</td>\n","      <td>2</td>\n","      <td>61.24</td>\n","      <td>280</td>\n","      <td>427</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2017-04-27</td>\n","      <td>3</td>\n","      <td>48.40</td>\n","      <td>325</td>\n","      <td>340</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2017-04-27</td>\n","      <td>4</td>\n","      <td>39.70</td>\n","      <td>345</td>\n","      <td>263</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2017-04-27</td>\n","      <td>5</td>\n","      <td>72.52</td>\n","      <td>398</td>\n","      <td>165</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72ecfeb8-101f-46f5-8147-5bfdaf5fa9be')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-72ecfeb8-101f-46f5-8147-5bfdaf5fa9be button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-72ecfeb8-101f-46f5-8147-5bfdaf5fa9be');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        Date  Station  Klorofil-a (µg/L)    X    Y\n","0 2017-04-27        1              86.14  235  537\n","1 2017-04-27        2              61.24  280  427\n","2 2017-04-27        3              48.40  325  340\n","3 2017-04-27        4              39.70  345  263\n","4 2017-04-27        5              72.52  398  165"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["features_filepath = '/content/drive/MyDrive/LakeRegression/340_Veri_toplam_temiz.xlsx'\n","feats_df = pd.read_excel(features_filepath)\n","selected_feature = 'Klorofil-a (µg/L)'\n","feats_df = feats_df[['Date', 'Station', selected_feature, 'X', 'Y']]\n","feats_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":221,"status":"ok","timestamp":1687180406312,"user":{"displayName":"Emirhan Böge","userId":"05794559765456875582"},"user_tz":-180},"id":"yD6vYl_9dsXj","outputId":"fabbd56d-9fa4-45b8-9831-5e384f1a156c"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-7c49c614-4c98-4a6e-bd0b-953114619c31\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Date</th>\n","      <th>Station</th>\n","      <th>Klorofil-a (µg/L)</th>\n","      <th>X</th>\n","      <th>Y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2017-04-27</td>\n","      <td>1</td>\n","      <td>2.643996</td>\n","      <td>235</td>\n","      <td>537</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2017-04-27</td>\n","      <td>2</td>\n","      <td>1.361115</td>\n","      <td>280</td>\n","      <td>427</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2017-04-27</td>\n","      <td>3</td>\n","      <td>0.699582</td>\n","      <td>325</td>\n","      <td>340</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2017-04-27</td>\n","      <td>4</td>\n","      <td>0.251347</td>\n","      <td>345</td>\n","      <td>263</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2017-04-27</td>\n","      <td>5</td>\n","      <td>1.942276</td>\n","      <td>398</td>\n","      <td>165</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c49c614-4c98-4a6e-bd0b-953114619c31')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7c49c614-4c98-4a6e-bd0b-953114619c31 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7c49c614-4c98-4a6e-bd0b-953114619c31');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["        Date  Station  Klorofil-a (µg/L)    X    Y\n","0 2017-04-27        1           2.643996  235  537\n","1 2017-04-27        2           1.361115  280  427\n","2 2017-04-27        3           0.699582  325  340\n","3 2017-04-27        4           0.251347  345  263\n","4 2017-04-27        5           1.942276  398  165"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["scaler = StandardScaler()\n","feats_df[selected_feature] = scaler.fit_transform(feats_df[selected_feature].values.reshape(-1, 1))\n","feats_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D7HMbOjgkYj8"},"outputs":[],"source":["from scipy.ndimage import zoom\n","\n","\n","desired_size = 224\n","\n","def pad_image(image, desired_size):\n","    pad_width = (desired_size - image.shape[-1]) // 2\n","\n","    if image.shape[-1] % 2 != 0:\n","        pad_width_1 = pad_width\n","        pad_width_2 = pad_width + 1\n","    else:\n","        pad_width_1 = pad_width\n","        pad_width_2 = pad_width\n","\n","    padded_image = np.pad(image, ((0, 0), (pad_width_1, pad_width_2), (pad_width_1, pad_width_2)))\n","    return padded_image\n","\n","\n","def resize_image(image, desired_size):\n","    scale_factors = [desired_size / dim for dim in image.shape[1:]]\n","    scale_factors = [1] + scale_factors\n","    resized_image = zoom(image, scale_factors)\n","    return resized_image"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106400,"status":"ok","timestamp":1687180512493,"user":{"displayName":"Emirhan Böge","userId":"05794559765456875582"},"user_tz":-180},"id":"Qs1zb9QFd4KL","outputId":"e8f2ae34-229d-4d30-c961-d2533b03d5a9"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 34/34 [00:01<00:00, 33.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train % 0.8125 | Val % 0.09375 | Test % 0.09375\n","Train shape: (260, 12, 224, 224) | (260,) | Val shape: (30, 12, 224, 224) | (30,) | Test shape: (30, 12, 224, 224) | (30,)\n"]}],"source":["unique_dates = feats_df['Date'].unique()\n","\n","use_padding = False\n","use_resize = True\n","\n","X_train = []\n","y_train = []\n","X_val = []\n","y_val = []\n","X_test = []\n","y_test = []\n","patch_size = 3\n","for i in tqdm(range(1, 35)):\n","    if i != 22 and i != 23:\n","        folder_path = f'/content/drive/MyDrive/LakeRegression/data/{i}'\n","        for j in range(10):\n","            file_path = f'{folder_path}/station_{j}_{patch_size}.h5'\n","            if os.path.isfile(file_path):\n","                with h5py.File(file_path, 'r') as f:\n","                    a_group_key = list(f.keys())[0]\n","                    image = np.array(f[a_group_key])\n","\n","                    unique_date = unique_dates[i - 1]\n","                    unique_station = j + 1\n","                    label = feats_df.loc[(feats_df['Date'] == unique_date) & (feats_df['Station'] == unique_station)][selected_feature].values[0]\n","\n","                    date = pd.to_datetime(unique_date)\n","                    if date < pd.to_datetime('2019-03-15'):\n","                        X_train.append(image)\n","                        y_train.append(label)\n","                    elif date >= pd.to_datetime('2019-03-15') and date < pd.to_datetime('2019-05-01'):\n","                        X_val.append(image)\n","                        y_val.append(label)\n","                    else:\n","                        X_test.append(image)\n","                        y_test.append(label)\n","\n","print(f'Train % {len(X_train) / (len(X_train) + len(X_val) + len(X_test))} | Val % {len(X_val) / (len(X_train) + len(X_val) + len(X_test))} | Test % {len(X_test) / (len(X_train) + len(X_val) + len(X_test))}')\n","\n","if use_padding:\n","  X_train = np.array([pad_image(x, desired_size) for x in X_train])\n","  X_val = np.array([pad_image(x, desired_size) for x in X_val])\n","  X_test = np.array([pad_image(x, desired_size) for x in X_test])\n","  y_train = np.array(y_train)\n","  y_val = np.array(y_val)\n","  y_test = np.array(y_test)\n","elif use_resize:\n","  X_train = np.array([resize_image(x, desired_size) for x in X_train])\n","  X_val = np.array([resize_image(x, desired_size) for x in X_val])\n","  X_test = np.array([resize_image(x, desired_size) for x in X_test])\n","  y_train = np.array(y_train)\n","  y_val = np.array(y_val)\n","  y_test = np.array(y_test)\n","\n","print(f'Train shape: {X_train.shape} | {y_train.shape} | Val shape: {X_val.shape} | {y_val.shape} | Test shape: {X_test.shape} | {y_test.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1413,"status":"ok","timestamp":1687181146805,"user":{"displayName":"Emirhan Böge","userId":"05794559765456875582"},"user_tz":-180},"id":"54sMgz0UeLge","outputId":"de90d890-3a70-4020-e33b-6d70766c635f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train loader: 9 | Val loader: 1 | Test loader: 1\n"]}],"source":["class LakeDataset(Dataset):\n","    def __init__(self, X, y, transform=None):\n","        self.X = X.astype(np.float32)\n","        self.y = y.astype(np.float32)\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.y)\n","\n","    def __getitem__(self, idx):\n","        image = self.X[idx]\n","        label = self.y[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label\n","\n","means = []\n","stds = []\n","for i in range(12):\n","    means.append(np.mean(X_train[:, i, :, :]))\n","    stds.append(np.std(X_train[:, i, :, :]))\n","\n","transform = transforms.Compose([\n","    transforms.Lambda(lambda x: torch.from_numpy(x).float()),\n","    transforms.Normalize(means, stds)\n","])\n","\n","train_dataset = LakeDataset(X_train, y_train, transform=transform)\n","val_dataset = LakeDataset(X_val, y_val, transform=transform)\n","test_dataset = LakeDataset(X_test, y_test, transform=transform)\n","\n","# use sequential sampler to preserve the date order\n","train_loader = DataLoader(train_dataset, batch_size=32, sampler=SequentialSampler(train_dataset))\n","val_loader = DataLoader(val_dataset, batch_size=32, sampler=SequentialSampler(val_dataset))\n","test_loader = DataLoader(test_dataset, batch_size=32, sampler=SequentialSampler(test_dataset))\n","\n","print(f'Train loader: {len(train_loader)} | Val loader: {len(val_loader)} | Test loader: {len(test_loader)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRGcl3e2em_A"},"outputs":[],"source":["class ViTBase16(nn.Module):\n","    def __init__(self, n_classes):\n","\n","        super(ViTBase16, self).__init__()\n","\n","        self.model = timm.create_model(\"vit_base_patch16_224\", pretrained=True, in_chans=12)\n","\n","        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n","\n","    def forward(self, x):\n","        x = self.model(x)\n","        return x\n","\n","    def train_one_epoch(self, train_loader, criterion, optimizer, device):\n","        epoch_loss = 0.0\n","        epoch_r2 = 0.0\n","        epoch_mse = 0.0\n","        epoch_rmse = 0.0\n","\n","        self.model.train()\n","        for i, (data, target) in enumerate(train_loader):\n","            if device.type == \"cuda\":\n","                data, target = data.cuda(), target.cuda()\n","            elif device.type == \"mps\":\n","                data, target = data.to(device), target.to(device)\n","            elif device.type == \"cpu\":\n","                data, target = data.cpu(), target.cpu()\n","\n","            optimizer.zero_grad()\n","\n","            output = self.forward(data)\n","\n","            loss = criterion(output, target)\n","            loss.backward()\n","\n","            r2 = r2_score(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n","            mse = mean_squared_error(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n","            rmse = np.sqrt(mse)\n","\n","            epoch_loss += loss\n","            epoch_r2 += r2\n","            epoch_mse += mse\n","            epoch_rmse += rmse\n","\n","            optimizer.step()\n","\n","        loss = epoch_loss / len(train_loader)\n","        r2 = epoch_r2 / len(train_loader)\n","        mse = epoch_mse / len(train_loader)\n","        rmse = epoch_rmse / len(train_loader)\n","\n","        return loss, r2, mse, rmse\n","\n","    def validate_one_epoch(self, valid_loader, criterion, device):\n","        valid_loss = 0.0\n","        valid_r2 = 0.0\n","        valid_mse = 0.0\n","        valid_rmse = 0.0\n","\n","        self.model.eval()\n","        for data, target in valid_loader:\n","            if device.type == \"cuda\":\n","                data, target = data.cuda(), target.cuda()\n","            elif device.type == \"mps\":\n","                data, target = data.to(device), target.to(device)\n","            elif device.type == \"cpu\":\n","                data, target = data.cpu(), target.cpu()\n","\n","            with torch.no_grad():\n","                output = self.model(data)\n","                loss = criterion(output, target)\n","\n","                r2 = r2_score(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n","                mse = mean_squared_error(target.cpu().detach().numpy(), output.cpu().detach().numpy())\n","                rmse = np.sqrt(mse)\n","\n","                valid_loss += loss\n","                valid_r2 += r2\n","                valid_mse += mse\n","                valid_rmse += rmse\n","\n","        loss = valid_loss / len(valid_loader)\n","        r2 = valid_r2 / len(valid_loader)\n","        mse = valid_mse / len(valid_loader)\n","        rmse = valid_rmse / len(valid_loader)\n","\n","        return loss, r2, mse, rmse\n","\n","if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","elif torch.backends.mps.is_available():\n","    device = torch.device('mps')\n","else:\n","    device = torch.device('cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288184,"status":"ok","timestamp":1687181438606,"user":{"displayName":"Emirhan Böge","userId":"05794559765456875582"},"user_tz":-180},"id":"t5o0nnLieoNs","outputId":"ed10f3f4-cd9e-4466-8b21-e29965906362"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 / 125\n","----------\n","Train loss: 107.29401397705078 | Train R2: -318.80994378839637 | Train MSE: 107.9126071876122 | Train RMSE: 7.843710190720028\n","Val loss: 1.4339667558670044 | Val R2: -12.66269795013437 | Val MSE: 1.4042556285858154 | Val RMSE: 1.185012936592102\n","----------\n","Epoch 2 / 125\n","----------\n","Train loss: 5.355320453643799 | Train R2: -42.07054336912409 | Train MSE: 5.411228193177117 | Train RMSE: 1.9677063491609361\n","Val loss: 1.9251153469085693 | Val R2: -17.551258048472583 | Val MSE: 1.9067028760910034 | Val RMSE: 1.3808341026306152\n","----------\n","Epoch 3 / 125\n","----------\n","Train loss: 2.6169605255126953 | Train R2: -51.476029845051016 | Train MSE: 2.418165382411745 | Train RMSE: 1.4038830796877544\n","Val loss: 2.900723457336426 | Val R2: -26.927552568661273 | Val MSE: 2.870400905609131 | Val RMSE: 1.694225788116455\n","----------\n","Epoch 4 / 125\n","----------\n","Train loss: 3.4589955806732178 | Train R2: -182.77229043242812 | Train MSE: 3.133231851789686 | Train RMSE: 1.6598583393626742\n","Val loss: 0.25607505440711975 | Val R2: -1.1936578686319526 | Val MSE: 0.22546467185020447 | Val RMSE: 0.4748311936855316\n","----------\n","Epoch 5 / 125\n","----------\n","Train loss: 1.765289545059204 | Train R2: -43.150717696179136 | Train MSE: 1.5811940398481157 | Train RMSE: 1.1424134373664856\n","Val loss: 0.47767868638038635 | Val R2: -3.2544140812950957 | Val MSE: 0.43726974725723267 | Val RMSE: 0.6612637639045715\n","----------\n","Epoch 6 / 125\n","----------\n","Train loss: 1.2139101028442383 | Train R2: -8.349401997017274 | Train MSE: 1.181905036378238 | Train RMSE: 0.948589661055141\n","Val loss: 0.16019226610660553 | Val R2: -0.046657283025329566 | Val MSE: 0.10757569968700409 | Val RMSE: 0.32798734307289124\n","----------\n","Epoch 7 / 125\n","----------\n","Train loss: 1.1658439636230469 | Train R2: -40.33477443694683 | Train MSE: 0.9881738788551755 | Train RMSE: 0.9365353650516934\n","Val loss: 0.26069629192352295 | Val R2: -0.9383388709503946 | Val MSE: 0.19922298192977905 | Val RMSE: 0.4463440179824829\n","----------\n","Epoch 8 / 125\n","----------\n","Train loss: 1.2969871759414673 | Train R2: -15.450403884557298 | Train MSE: 1.208614198697938 | Train RMSE: 0.9561430712540945\n","Val loss: 0.22590915858745575 | Val R2: -0.45598619461254697 | Val MSE: 0.1496466100215912 | Val RMSE: 0.3868418335914612\n","----------\n","Epoch 9 / 125\n","----------\n","Train loss: 0.9047298431396484 | Train R2: -5.90751581454963 | Train MSE: 0.7861637791825665 | Train RMSE: 0.7954819020297792\n","Val loss: 0.15890392661094666 | Val R2: 0.3164895052165293 | Val MSE: 0.07025138288736343 | Val RMSE: 0.2650497853755951\n","----------\n","Epoch 10 / 125\n","----------\n","Train loss: 1.240024447441101 | Train R2: -14.87217163697482 | Train MSE: 1.1266833552055888 | Train RMSE: 0.9404676655928293\n","Val loss: 0.2200486809015274 | Val R2: -0.25014564571681586 | Val MSE: 0.12849026918411255 | Val RMSE: 0.3584553897380829\n","----------\n","Epoch 11 / 125\n","----------\n","Train loss: 0.9460190534591675 | Train R2: -5.694192496072571 | Train MSE: 0.8231892635424932 | Train RMSE: 0.8080789794524511\n","Val loss: 0.19463369250297546 | Val R2: 0.10244687199831792 | Val MSE: 0.09225073456764221 | Val RMSE: 0.3037280738353729\n","----------\n","Epoch 12 / 125\n","----------\n","Train loss: 1.1215710639953613 | Train R2: -11.840946332217564 | Train MSE: 1.0187415555119514 | Train RMSE: 0.8952161305480533\n","Val loss: 0.3104912042617798 | Val R2: -1.0321406558577082 | Val MSE: 0.20886392891407013 | Val RMSE: 0.45701631903648376\n","----------\n","Epoch 13 / 125\n","----------\n","Train loss: 0.9092145562171936 | Train R2: -3.2954839692196494 | Train MSE: 0.80740660842922 | Train RMSE: 0.7862739521596167\n","Val loss: 0.24829871952533722 | Val R2: -0.42622884906828507 | Val MSE: 0.14658814668655396 | Val RMSE: 0.38286831974983215\n","----------\n","Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n","Epoch 14 / 125\n","----------\n","Train loss: 1.0151268243789673 | Train R2: -1.8109595036796517 | Train MSE: 1.0189942204289966 | Train RMSE: 0.8359718339310752\n","Val loss: 0.23254907131195068 | Val R2: -0.2839613029939143 | Val MSE: 0.13196586072444916 | Val RMSE: 0.3632710576057434\n","----------\n","Epoch 15 / 125\n","----------\n","Train loss: 0.8716064691543579 | Train R2: -2.668965332043889 | Train MSE: 0.8270094329749959 | Train RMSE: 0.78158097093304\n","Val loss: 0.25950726866722107 | Val R2: -0.5795652740653254 | Val MSE: 0.16234812140464783 | Val RMSE: 0.40292444825172424\n","----------\n","Epoch 16 / 125\n","----------\n","Train loss: 0.8088570237159729 | Train R2: -4.3761824213909035 | Train MSE: 0.7314471463776298 | Train RMSE: 0.7670699167582724\n","Val loss: 0.29691582918167114 | Val R2: -0.9717622400106556 | Val MSE: 0.20265819132328033 | Val RMSE: 0.4501757323741913\n","----------\n","Epoch 17 / 125\n","----------\n","Train loss: 0.7892840504646301 | Train R2: -4.796318735609568 | Train MSE: 0.704412580985162 | Train RMSE: 0.761445021463765\n","Val loss: 0.2992126941680908 | Val R2: -1.0085946735853693 | Val MSE: 0.20644386112689972 | Val RMSE: 0.4543609321117401\n","----------\n","Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n","Epoch 18 / 125\n","----------\n","Train loss: 0.7711519598960876 | Train R2: -4.2870923594121875 | Train MSE: 0.6933066086429689 | Train RMSE: 0.7527356056703461\n","Val loss: 0.2965129315853119 | Val R2: -0.9829778158233817 | Val MSE: 0.2038109451532364 | Val RMSE: 0.4514542520046234\n","----------\n","Epoch 19 / 125\n","----------\n","Train loss: 0.7703092098236084 | Train R2: -4.166831752635307 | Train MSE: 0.6929082526928849 | Train RMSE: 0.7515790238976479\n","Val loss: 0.29331639409065247 | Val R2: -0.9524013420921624 | Val MSE: 0.20066829025745392 | Val RMSE: 0.44796013832092285\n","----------\n","Epoch 20 / 125\n","----------\n","Train loss: 0.7696666121482849 | Train R2: -4.051146991152249 | Train MSE: 0.6927254796028137 | Train RMSE: 0.7504968121647835\n","Val loss: 0.2902589440345764 | Val R2: -0.9232130887216492 | Val MSE: 0.19766831398010254 | Val RMSE: 0.44459906220436096\n","----------\n","Epoch 21 / 125\n","----------\n","Train loss: 0.7691878080368042 | Train R2: -3.9499499777246196 | Train MSE: 0.6927179876301024 | Train RMSE: 0.7495700178874863\n","Val loss: 0.2875816226005554 | Val R2: -0.8978242899667748 | Val MSE: 0.19505883753299713 | Val RMSE: 0.44165465235710144\n","----------\n","Epoch 00021: reducing learning rate of group 0 to 1.0000e-06.\n","Epoch 22 / 125\n","----------\n","Train loss: 0.7685173153877258 | Train R2: -3.900117338359298 | Train MSE: 0.692838429990742 | Train RMSE: 0.7490514972143703\n","Val loss: 0.2873491048812866 | Val R2: -0.8956390135688486 | Val MSE: 0.19483424723148346 | Val RMSE: 0.44140031933784485\n","----------\n","Epoch 23 / 125\n","----------\n","Train loss: 0.7684850096702576 | Train R2: -3.8923817591406547 | Train MSE: 0.6928513063531783 | Train RMSE: 0.7489831381373935\n","Val loss: 0.2871359586715698 | Val R2: -0.893650399625771 | Val MSE: 0.19462984800338745 | Val RMSE: 0.44116872549057007\n","----------\n","Epoch 24 / 125\n","----------\n","Train loss: 0.7684503197669983 | Train R2: -3.884985537418484 | Train MSE: 0.6928611279775699 | Train RMSE: 0.7489166855812073\n","Val loss: 0.2869308292865753 | Val R2: -0.8917437381681703 | Val MSE: 0.19443389773368835 | Val RMSE: 0.4409465789794922\n","----------\n","Epoch 25 / 125\n","----------\n","Train loss: 0.7684146761894226 | Train R2: -3.877736517806295 | Train MSE: 0.6928703017118905 | Train RMSE: 0.7488511817322837\n","Val loss: 0.2867286503314972 | Val R2: -0.889869878255032 | Val MSE: 0.19424130022525787 | Val RMSE: 0.44072815775871277\n","----------\n","Epoch 00025: reducing learning rate of group 0 to 1.0000e-07.\n","Epoch 26 / 125\n","----------\n","Train loss: 0.7683262228965759 | Train R2: -3.8729643601753523 | Train MSE: 0.6928622770226665 | Train RMSE: 0.7487948404418098\n","Val loss: 0.2867087125778198 | Val R2: -0.8896846396066294 | Val MSE: 0.19422227144241333 | Val RMSE: 0.4407065510749817\n","----------\n","Epoch 27 / 125\n","----------\n","Train loss: 0.768322765827179 | Train R2: -3.8722587951102065 | Train MSE: 0.6928629444705116 | Train RMSE: 0.7487884139021238\n","Val loss: 0.2866886556148529 | Val R2: -0.8894990393935105 | Val MSE: 0.194203183054924 | Val RMSE: 0.4406849145889282\n","----------\n","Epoch 28 / 125\n","----------\n","Train loss: 0.7683193683624268 | Train R2: -3.871540124162996 | Train MSE: 0.6928635437248482 | Train RMSE: 0.7487818251053492\n","Val loss: 0.2866683006286621 | Val R2: -0.8893103061984344 | Val MSE: 0.19418379664421082 | Val RMSE: 0.4406629204750061\n","----------\n","Epoch 29 / 125\n","----------\n","Train loss: 0.7683156132698059 | Train R2: -3.870817256977645 | Train MSE: 0.6928641481531991 | Train RMSE: 0.7487751924329333\n","Val loss: 0.2866475582122803 | Val R2: -0.8891183228474759 | Val MSE: 0.19416403770446777 | Val RMSE: 0.44064047932624817\n","----------\n","Epoch 00029: reducing learning rate of group 0 to 1.0000e-08.\n","Epoch 30 / 125\n","----------\n","Train loss: 0.7683064937591553 | Train R2: -3.870350933137628 | Train MSE: 0.6928629771702819 | Train RMSE: 0.7487694712148772\n","Val loss: 0.28664669394493103 | Val R2: -0.8891105916337241 | Val MSE: 0.1941632628440857 | Val RMSE: 0.4406396150588989\n","----------\n","Epoch 31 / 125\n","----------\n","Train loss: 0.7683062553405762 | Train R2: -3.8703166791691532 | Train MSE: 0.6928629970384969 | Train RMSE: 0.7487691524955962\n","Val loss: 0.2866457998752594 | Val R2: -0.8891019232739792 | Val MSE: 0.19416235387325287 | Val RMSE: 0.44063857197761536\n","----------\n","Epoch 32 / 125\n","----------\n","Train loss: 0.7683060765266418 | Train R2: -3.8702808861675706 | Train MSE: 0.6928630978282955 | Train RMSE: 0.7487688519888454\n","Val loss: 0.28664496541023254 | Val R2: -0.8890945974589783 | Val MSE: 0.19416159391403198 | Val RMSE: 0.4406377077102661\n","----------\n","Epoch 33 / 125\n","----------\n","Train loss: 0.7683059573173523 | Train R2: -3.8702468815650013 | Train MSE: 0.6928631878561444 | Train RMSE: 0.7487685564491484\n","Val loss: 0.28664401173591614 | Val R2: -0.889085888908596 | Val MSE: 0.19416071474552155 | Val RMSE: 0.4406367242336273\n","----------\n","Epoch 34 / 125\n","----------\n","Train loss: 0.7683057188987732 | Train R2: -3.870214061955034 | Train MSE: 0.6928632142436173 | Train RMSE: 0.7487682418690788\n","Val loss: 0.2866430878639221 | Val R2: -0.8890772584552862 | Val MSE: 0.1941598355770111 | Val RMSE: 0.44063571095466614\n","----------\n","Epoch 35 / 125\n","----------\n","Train loss: 0.7683054804801941 | Train R2: -3.8701781726191253 | Train MSE: 0.6928632739517424 | Train RMSE: 0.7487679107321633\n","Val loss: 0.2866421341896057 | Val R2: -0.889068464804202 | Val MSE: 0.19415892660617828 | Val RMSE: 0.44063469767570496\n","----------\n","Epoch 36 / 125\n","----------\n","Train loss: 0.768305242061615 | Train R2: -3.870139665739297 | Train MSE: 0.692863366049197 | Train RMSE: 0.7487675853901439\n","Val loss: 0.28664112091064453 | Val R2: -0.8890594483395458 | Val MSE: 0.19415800273418427 | Val RMSE: 0.440633624792099\n","----------\n","Epoch 37 / 125\n","----------\n","Train loss: 0.7683050632476807 | Train R2: -3.8701028163761424 | Train MSE: 0.6928633836408457 | Train RMSE: 0.748767232729329\n","Val loss: 0.2866401672363281 | Val R2: -0.8890506177447897 | Val MSE: 0.19415709376335144 | Val RMSE: 0.4406326115131378\n","----------\n","Epoch 38 / 125\n","----------\n","Train loss: 0.7683048248291016 | Train R2: -3.870063809005092 | Train MSE: 0.6928634927090671 | Train RMSE: 0.7487669164935747\n","Val loss: 0.2866390645503998 | Val R2: -0.8890406213950124 | Val MSE: 0.19415606558322906 | Val RMSE: 0.4406314492225647\n","----------\n","Epoch 39 / 125\n","----------\n","Train loss: 0.7683046460151672 | Train R2: -3.87002421857607 | Train MSE: 0.6928635432074467 | Train RMSE: 0.7487665389974912\n","Val loss: 0.2866379916667938 | Val R2: -0.889030656080185 | Val MSE: 0.1941550374031067 | Val RMSE: 0.4406302869319916\n","----------\n","Epoch 40 / 125\n","----------\n","Train loss: 0.7683044075965881 | Train R2: -3.8699824855774785 | Train MSE: 0.6928636146088442 | Train RMSE: 0.7487661631570922\n","Val loss: 0.2866368591785431 | Val R2: -0.8890203442084181 | Val MSE: 0.19415397942066193 | Val RMSE: 0.4406290650367737\n","----------\n","Epoch 41 / 125\n","----------\n","Train loss: 0.7683041095733643 | Train R2: -3.8699415311510927 | Train MSE: 0.6928636611749729 | Train RMSE: 0.7487657757269012\n","Val loss: 0.28663578629493713 | Val R2: -0.8890103113302785 | Val MSE: 0.19415293633937836 | Val RMSE: 0.44062790274620056\n","----------\n","Epoch 42 / 125\n","----------\n","Train loss: 0.7683038711547852 | Train R2: -3.869899357583679 | Train MSE: 0.6928637555489937 | Train RMSE: 0.7487654123041365\n","Val loss: 0.2866346538066864 | Val R2: -0.8889999547561025 | Val MSE: 0.1941518783569336 | Val RMSE: 0.44062668085098267\n","----------\n","Epoch 43 / 125\n","----------\n","Train loss: 0.768303632736206 | Train R2: -3.869856821306783 | Train MSE: 0.6928638043916888 | Train RMSE: 0.7487650199068917\n","Val loss: 0.28663352131843567 | Val R2: -0.8889895284099223 | Val MSE: 0.19415079057216644 | Val RMSE: 0.44062545895576477\n","----------\n","Epoch 44 / 125\n","----------\n","Train loss: 0.768303394317627 | Train R2: -3.869815257883222 | Train MSE: 0.6928638613058461 | Train RMSE: 0.7487646217147509\n","Val loss: 0.2866322994232178 | Val R2: -0.8889783389702048 | Val MSE: 0.1941496580839157 | Val RMSE: 0.4406241774559021\n","----------\n","Epoch 45 / 125\n","----------\n","Train loss: 0.7683030962944031 | Train R2: -3.869769835475441 | Train MSE: 0.6928639941745334 | Train RMSE: 0.748764251669248\n","Val loss: 0.28663110733032227 | Val R2: -0.8889670101563789 | Val MSE: 0.1941484957933426 | Val RMSE: 0.44062283635139465\n","----------\n","Epoch 46 / 125\n","----------\n","Train loss: 0.768302857875824 | Train R2: -3.8697231925357554 | Train MSE: 0.6928640939295292 | Train RMSE: 0.7487638377481036\n","Val loss: 0.28662991523742676 | Val R2: -0.8889565268750355 | Val MSE: 0.19414740800857544 | Val RMSE: 0.44062161445617676\n","----------\n","Epoch 47 / 125\n","----------\n","Train loss: 0.7683026790618896 | Train R2: -3.869677831619489 | Train MSE: 0.6928641745406721 | Train RMSE: 0.7487634321053823\n","Val loss: 0.28662869334220886 | Val R2: -0.8889451910575803 | Val MSE: 0.1941462904214859 | Val RMSE: 0.4406203329563141\n","----------\n","Epoch 48 / 125\n","----------\n","Train loss: 0.7683023810386658 | Train R2: -3.8696328952814514 | Train MSE: 0.6928642288678222 | Train RMSE: 0.748763006594446\n","Val loss: 0.2866274118423462 | Val R2: -0.8889334534470161 | Val MSE: 0.1941450536251068 | Val RMSE: 0.44061893224716187\n","----------\n","Epoch 49 / 125\n","----------\n","Train loss: 0.7683020234107971 | Train R2: -3.869582494386753 | Train MSE: 0.6928643137216568 | Train RMSE: 0.7487625579039255\n","Val loss: 0.2866261303424835 | Val R2: -0.8889215496559999 | Val MSE: 0.19414381682872772 | Val RMSE: 0.44061753153800964\n","----------\n","Epoch 50 / 125\n","----------\n","Train loss: 0.7683019042015076 | Train R2: -3.869534766872496 | Train MSE: 0.6928643888483444 | Train RMSE: 0.7487621208031973\n","Val loss: 0.2866247296333313 | Val R2: -0.8889092808456471 | Val MSE: 0.19414256513118744 | Val RMSE: 0.4406161308288574\n","----------\n","Epoch 51 / 125\n","----------\n","Train loss: 0.7683014273643494 | Train R2: -3.8694841499428225 | Train MSE: 0.6928644602497419 | Train RMSE: 0.7487616580393579\n","Val loss: 0.28662341833114624 | Val R2: -0.8888967340044303 | Val MSE: 0.19414125382900238 | Val RMSE: 0.44061464071273804\n","----------\n","Epoch 52 / 125\n","----------\n","Train loss: 0.768301248550415 | Train R2: -3.869434544747461 | Train MSE: 0.6928645931184292 | Train RMSE: 0.7487612242499987\n","Val loss: 0.28662198781967163 | Val R2: -0.8888841116712645 | Val MSE: 0.1941399872303009 | Val RMSE: 0.44061318039894104\n","----------\n","Epoch 53 / 125\n","----------\n","Train loss: 0.7683010697364807 | Train R2: -3.869386852033009 | Train MSE: 0.6928646467212174 | Train RMSE: 0.7487608028782738\n","Val loss: 0.2866205871105194 | Val R2: -0.888871174401318 | Val MSE: 0.19413867592811584 | Val RMSE: 0.44061172008514404\n","----------\n","Epoch 54 / 125\n","----------\n","Train loss: 0.7683007717132568 | Train R2: -3.869329877199015 | Train MSE: 0.6928647249523137 | Train RMSE: 0.7487602995501624\n","Val loss: 0.2866192162036896 | Val R2: -0.8888587664811447 | Val MSE: 0.1941373646259308 | Val RMSE: 0.44061022996902466\n","----------\n","Epoch 55 / 125\n","----------\n","Train loss: 0.7683004140853882 | Train R2: -3.869280711614623 | Train MSE: 0.692864803597331 | Train RMSE: 0.7487598243686888\n","Val loss: 0.28661778569221497 | Val R2: -0.8888454214149784 | Val MSE: 0.19413597881793976 | Val RMSE: 0.4406086504459381\n","----------\n","Epoch 56 / 125\n","----------\n","Train loss: 0.7683001160621643 | Train R2: -3.8692234280159616 | Train MSE: 0.6928648689968718 | Train RMSE: 0.7487593102786276\n","Val loss: 0.28661635518074036 | Val R2: -0.8888322943108136 | Val MSE: 0.1941346824169159 | Val RMSE: 0.4406071603298187\n","----------\n","Epoch 57 / 125\n","----------\n","Train loss: 0.7682997584342957 | Train R2: -3.869171266525034 | Train MSE: 0.6928649611978067 | Train RMSE: 0.7487588251630465\n","Val loss: 0.2866148352622986 | Val R2: -0.88881843803635 | Val MSE: 0.1941332221031189 | Val RMSE: 0.4406055212020874\n","----------\n","Epoch 58 / 125\n","----------\n","Train loss: 0.7682994604110718 | Train R2: -3.8691169512766788 | Train MSE: 0.692865057537953 | Train RMSE: 0.7487583408753077\n","Val loss: 0.2866133749485016 | Val R2: -0.888805096821236 | Val MSE: 0.19413186609745026 | Val RMSE: 0.44060397148132324\n","----------\n","Epoch 59 / 125\n","----------\n","Train loss: 0.7682991027832031 | Train R2: -3.869062368332324 | Train MSE: 0.6928651256279813 | Train RMSE: 0.7487578516205152\n","Val loss: 0.2866118848323822 | Val R2: -0.8887915828562278 | Val MSE: 0.19413048028945923 | Val RMSE: 0.4406023919582367\n","----------\n","Epoch 60 / 125\n","----------\n","Train loss: 0.768298864364624 | Train R2: -3.8690054033746546 | Train MSE: 0.6928652033416761 | Train RMSE: 0.7487573300798734\n","Val loss: 0.2866103947162628 | Val R2: -0.8887773131222325 | Val MSE: 0.19412900507450104 | Val RMSE: 0.440600723028183\n","----------\n","Epoch 61 / 125\n","----------\n","Train loss: 0.7682985067367554 | Train R2: -3.8689479874181925 | Train MSE: 0.6928653073393636 | Train RMSE: 0.7487568275796043\n","Val loss: 0.2866087555885315 | Val R2: -0.8887630873732939 | Val MSE: 0.19412754476070404 | Val RMSE: 0.44059908390045166\n","----------\n","Epoch 62 / 125\n","----------\n","Train loss: 0.7682982087135315 | Train R2: -3.868892488329286 | Train MSE: 0.6928653677718507 | Train RMSE: 0.7487562795480093\n","Val loss: 0.28660720586776733 | Val R2: -0.8887486706385967 | Val MSE: 0.19412605464458466 | Val RMSE: 0.44059738516807556\n","----------\n","Epoch 63 / 125\n","----------\n","Train loss: 0.7682979106903076 | Train R2: -3.868832009856681 | Train MSE: 0.6928654491073556 | Train RMSE: 0.7487557563516829\n","Val loss: 0.2866056263446808 | Val R2: -0.8887341223074061 | Val MSE: 0.19412454962730408 | Val RMSE: 0.44059568643569946\n","----------\n","Epoch 64 / 125\n","----------\n","Train loss: 0.7682974934577942 | Train R2: -3.8687696260692914 | Train MSE: 0.6928655220609572 | Train RMSE: 0.7487551834848192\n","Val loss: 0.28660398721694946 | Val R2: -0.8887188511110713 | Val MSE: 0.19412298500537872 | Val RMSE: 0.4405938982963562\n","----------\n","Epoch 65 / 125\n","----------\n","Train loss: 0.7682971954345703 | Train R2: -3.8687098148808494 | Train MSE: 0.6928656353718705 | Train RMSE: 0.748754657804966\n","Val loss: 0.2866023778915405 | Val R2: -0.8887039010811602 | Val MSE: 0.19412145018577576 | Val RMSE: 0.4405921697616577\n","----------\n","Epoch 66 / 125\n","----------\n","Train loss: 0.7682968974113464 | Train R2: -3.8686493766751937 | Train MSE: 0.6928657513732711 | Train RMSE: 0.7487541089455286\n","Val loss: 0.28660061955451965 | Val R2: -0.8886879345811325 | Val MSE: 0.19411982595920563 | Val RMSE: 0.4405903220176697\n","----------\n","Epoch 67 / 125\n","----------\n","Train loss: 0.7682965397834778 | Train R2: -3.868588763328055 | Train MSE: 0.692865807770027 | Train RMSE: 0.7487535468406148\n","Val loss: 0.28659895062446594 | Val R2: -0.8886724180689991 | Val MSE: 0.1941182017326355 | Val RMSE: 0.44058847427368164\n","----------\n","Epoch 68 / 125\n","----------\n","Train loss: 0.7682961225509644 | Train R2: -3.868526682226762 | Train MSE: 0.6928659391899904 | Train RMSE: 0.7487529988090197\n","Val loss: 0.2865971624851227 | Val R2: -0.8886564133982484 | Val MSE: 0.19411659240722656 | Val RMSE: 0.440586656332016\n","----------\n","Epoch 69 / 125\n","----------\n","Train loss: 0.7682957649230957 | Train R2: -3.8684615492644068 | Train MSE: 0.6928660141097175 | Train RMSE: 0.7487523994512029\n","Val loss: 0.2865954041481018 | Val R2: -0.8886400533546694 | Val MSE: 0.19411487877368927 | Val RMSE: 0.4405846893787384\n","----------\n","Epoch 70 / 125\n","----------\n","Train loss: 0.7682955265045166 | Train R2: -3.868396234615178 | Train MSE: 0.6928660691612296 | Train RMSE: 0.7487517868479093\n","Val loss: 0.28659364581108093 | Val R2: -0.8886233622527484 | Val MSE: 0.19411318004131317 | Val RMSE: 0.4405827820301056\n","----------\n","Epoch 71 / 125\n","----------\n","Train loss: 0.7682950496673584 | Train R2: -3.868330467148767 | Train MSE: 0.6928661636387309 | Train RMSE: 0.7487511990798844\n","Val loss: 0.2865917980670929 | Val R2: -0.8886069203931886 | Val MSE: 0.19411149621009827 | Val RMSE: 0.4405808746814728\n","----------\n","Epoch 72 / 125\n","----------\n","Train loss: 0.7682946920394897 | Train R2: -3.8682647326983934 | Train MSE: 0.6928662755009201 | Train RMSE: 0.7487505964106984\n","Val loss: 0.28658998012542725 | Val R2: -0.8885896922205123 | Val MSE: 0.1941097229719162 | Val RMSE: 0.4405788481235504\n","----------\n","Epoch 73 / 125\n","----------\n","Train loss: 0.7682942748069763 | Train R2: -3.8681966139352366 | Train MSE: 0.6928663987459408 | Train RMSE: 0.7487500028477775\n","Val loss: 0.28658804297447205 | Val R2: -0.8885723847048348 | Val MSE: 0.19410796463489532 | Val RMSE: 0.44057685136795044\n","----------\n","Epoch 74 / 125\n","----------\n","Train loss: 0.7682939767837524 | Train R2: -3.8681275263915467 | Train MSE: 0.6928664710786607 | Train RMSE: 0.748749370376269\n","Val loss: 0.28658613562583923 | Val R2: -0.8885549455549084 | Val MSE: 0.1941061168909073 | Val RMSE: 0.4405747652053833\n","----------\n","Epoch 75 / 125\n","----------\n","Train loss: 0.7682936191558838 | Train R2: -3.868058491502019 | Train MSE: 0.6928665701092945 | Train RMSE: 0.7487487387326028\n","Val loss: 0.28658419847488403 | Val R2: -0.8885375040263912 | Val MSE: 0.19410432875156403 | Val RMSE: 0.44057273864746094\n","----------\n","Epoch 76 / 125\n","----------\n","Train loss: 0.7682932615280151 | Train R2: -3.8679909803073067 | Train MSE: 0.6928666650007168 | Train RMSE: 0.7487481203344133\n","Val loss: 0.2865823209285736 | Val R2: -0.8885200922680172 | Val MSE: 0.19410255551338196 | Val RMSE: 0.4405707120895386\n","----------\n","Epoch 77 / 125\n","----------\n","Train loss: 0.7682928442955017 | Train R2: -3.8679186832600427 | Train MSE: 0.6928667705506086 | Train RMSE: 0.7487474646833208\n","Val loss: 0.286580353975296 | Val R2: -0.888501866635075 | Val MSE: 0.19410069286823273 | Val RMSE: 0.44056859612464905\n","----------\n","Epoch 78 / 125\n","----------\n","Train loss: 0.7682924270629883 | Train R2: -3.867847631179111 | Train MSE: 0.6928668330527015 | Train RMSE: 0.748746805720859\n","Val loss: 0.28657838702201843 | Val R2: -0.8884837927788944 | Val MSE: 0.1940988153219223 | Val RMSE: 0.4405664801597595\n","----------\n","Epoch 79 / 125\n","----------\n","Train loss: 0.7682920694351196 | Train R2: -3.8677761111963793 | Train MSE: 0.6928669803051485 | Train RMSE: 0.7487461732493507\n","Val loss: 0.28657639026641846 | Val R2: -0.8884655707327165 | Val MSE: 0.19409698247909546 | Val RMSE: 0.4405643939971924\n","----------\n","Epoch 80 / 125\n","----------\n","Train loss: 0.7682915925979614 | Train R2: -3.8676997249826623 | Train MSE: 0.6928670783009794 | Train RMSE: 0.7487455026970969\n","Val loss: 0.2865743339061737 | Val R2: -0.8884469972591509 | Val MSE: 0.19409504532814026 | Val RMSE: 0.4405621886253357\n","----------\n","Epoch 81 / 125\n","----------\n","Train loss: 0.7682912945747375 | Train R2: -3.8676272332746544 | Train MSE: 0.6928671565320756 | Train RMSE: 0.7487448131044706\n","Val loss: 0.28657233715057373 | Val R2: -0.8884280230685053 | Val MSE: 0.19409307837486267 | Val RMSE: 0.4405599534511566\n","----------\n","Epoch 82 / 125\n","----------\n","Train loss: 0.7682908773422241 | Train R2: -3.8675527587364886 | Train MSE: 0.6928672852615515 | Train RMSE: 0.7487441574533781\n","Val loss: 0.2865701913833618 | Val R2: -0.8884085112974065 | Val MSE: 0.19409111142158508 | Val RMSE: 0.44055771827697754\n","----------\n","Epoch 83 / 125\n","----------\n","Train loss: 0.7682904601097107 | Train R2: -3.867476754549485 | Train MSE: 0.6928673591464758 | Train RMSE: 0.7487434388862716\n","Val loss: 0.2865680754184723 | Val R2: -0.8883895092243883 | Val MSE: 0.1940891444683075 | Val RMSE: 0.44055548310279846\n","----------\n","Epoch 84 / 125\n","----------\n","Train loss: 0.7682900428771973 | Train R2: -3.8673986118072645 | Train MSE: 0.6928674574527476 | Train RMSE: 0.7487427501214875\n","Val loss: 0.28656595945358276 | Val R2: -0.8883697407542339 | Val MSE: 0.19408710300922394 | Val RMSE: 0.4405531883239746\n","----------\n","Epoch 85 / 125\n","----------\n","Train loss: 0.7682896852493286 | Train R2: -3.8673198911924214 | Train MSE: 0.6928675526546108 | Train RMSE: 0.7487420348657502\n","Val loss: 0.28656378388404846 | Val R2: -0.888349411616191 | Val MSE: 0.1940850168466568 | Val RMSE: 0.4405508041381836\n","----------\n","Epoch 86 / 125\n","----------\n","Train loss: 0.7682892680168152 | Train R2: -3.8672436720915013 | Train MSE: 0.6928676443381442 | Train RMSE: 0.7487413295441203\n","Val loss: 0.2865615487098694 | Val R2: -0.8883292227395088 | Val MSE: 0.19408294558525085 | Val RMSE: 0.44054844975471497\n","----------\n","Epoch 87 / 125\n","----------\n","Train loss: 0.7682888507843018 | Train R2: -3.8671637516158217 | Train MSE: 0.6928677280536957 | Train RMSE: 0.7487406002150642\n","Val loss: 0.2865593731403351 | Val R2: -0.8883090956118049 | Val MSE: 0.19408084452152252 | Val RMSE: 0.44054606556892395\n","----------\n","Epoch 88 / 125\n","----------\n","Train loss: 0.7682884335517883 | Train R2: -3.8670859757390588 | Train MSE: 0.6928678743748201 | Train RMSE: 0.7487399064832263\n","Val loss: 0.2865571081638336 | Val R2: -0.8882884412863867 | Val MSE: 0.1940787434577942 | Val RMSE: 0.44054368138313293\n","----------\n","Epoch 89 / 125\n","----------\n","Train loss: 0.7682879567146301 | Train R2: -3.8670032053391927 | Train MSE: 0.6928679763029019 | Train RMSE: 0.7487391614251666\n","Val loss: 0.28655484318733215 | Val R2: -0.888267857393692 | Val MSE: 0.19407664239406586 | Val RMSE: 0.4405412971973419\n","----------\n","Epoch 90 / 125\n","----------\n","Train loss: 0.7682874798774719 | Train R2: -3.866923542818784 | Train MSE: 0.692868018626339 | Train RMSE: 0.7487383940153651\n","Val loss: 0.2865526080131531 | Val R2: -0.8882468179819349 | Val MSE: 0.19407446682453156 | Val RMSE: 0.44053882360458374\n","----------\n","Epoch 91 / 125\n","----------\n","Train loss: 0.7682870626449585 | Train R2: -3.8668386244697555 | Train MSE: 0.6928681217961841 | Train RMSE: 0.748737619154983\n","Val loss: 0.28655022382736206 | Val R2: -0.8882252117292793 | Val MSE: 0.19407224655151367 | Val RMSE: 0.4405363202095032\n","----------\n","Epoch 92 / 125\n","----------\n","Train loss: 0.7682865858078003 | Train R2: -3.8667569320102597 | Train MSE: 0.6928682252764702 | Train RMSE: 0.7487368823753463\n","Val loss: 0.2865479290485382 | Val R2: -0.8882036666026378 | Val MSE: 0.19407004117965698 | Val RMSE: 0.4405338168144226\n","----------\n","Epoch 93 / 125\n","----------\n","Train loss: 0.7682861089706421 | Train R2: -3.8666717484357758 | Train MSE: 0.6928683486249712 | Train RMSE: 0.7487361157933871\n","Val loss: 0.2865455448627472 | Val R2: -0.8881821037687074 | Val MSE: 0.1940678209066391 | Val RMSE: 0.44053128361701965\n","----------\n","Epoch 94 / 125\n","----------\n","Train loss: 0.7682857513427734 | Train R2: -3.866587941596287 | Train MSE: 0.6928684107131429 | Train RMSE: 0.7487353376216359\n","Val loss: 0.2865431308746338 | Val R2: -0.8881601274752453 | Val MSE: 0.19406555593013763 | Val RMSE: 0.4405287206172943\n","----------\n","Epoch 95 / 125\n","----------\n","Train loss: 0.76828533411026 | Train R2: -3.866500666053357 | Train MSE: 0.6928684467242824 | Train RMSE: 0.7487345238526663\n","Val loss: 0.2865407168865204 | Val R2: -0.8881377114256059 | Val MSE: 0.1940632462501526 | Val RMSE: 0.4405260980129242\n","----------\n","Epoch 96 / 125\n","----------\n","Train loss: 0.7682848572731018 | Train R2: -3.8664142167427045 | Train MSE: 0.6928685957358943 | Train RMSE: 0.748733739886019\n","Val loss: 0.2865382432937622 | Val R2: -0.8881152469546487 | Val MSE: 0.19406096637248993 | Val RMSE: 0.44052350521087646\n","----------\n","Epoch 97 / 125\n","----------\n","Train loss: 0.7682843208312988 | Train R2: -3.8663266013056137 | Train MSE: 0.692868708115485 | Train RMSE: 0.7487329542636871\n","Val loss: 0.2865357995033264 | Val R2: -0.8880922877367547 | Val MSE: 0.1940585821866989 | Val RMSE: 0.4405207931995392\n","----------\n","Epoch 98 / 125\n","----------\n","Train loss: 0.7682839035987854 | Train R2: -3.866235919981798 | Train MSE: 0.6928688194602728 | Train RMSE: 0.7487321264213986\n","Val loss: 0.28653326630592346 | Val R2: -0.8880694492417951 | Val MSE: 0.19405624270439148 | Val RMSE: 0.4405181407928467\n","----------\n","Epoch 99 / 125\n","----------\n","Train loss: 0.7682834267616272 | Train R2: -3.866147447265764 | Train MSE: 0.692868891172111 | Train RMSE: 0.7487313093410598\n","Val loss: 0.2865307629108429 | Val R2: -0.888045840385381 | Val MSE: 0.19405381381511688 | Val RMSE: 0.440515398979187\n","----------\n","Epoch 100 / 125\n","----------\n","Train loss: 0.768282949924469 | Train R2: -3.866058420335883 | Train MSE: 0.6928689860635333 | Train RMSE: 0.7487304723925061\n","Val loss: 0.28652819991111755 | Val R2: -0.8880221656646483 | Val MSE: 0.19405139982700348 | Val RMSE: 0.44051265716552734\n","----------\n","Epoch 101 / 125\n","----------\n","Train loss: 0.7682824730873108 | Train R2: -3.865965531595344 | Train MSE: 0.6928690758844217 | Train RMSE: 0.7487296362717947\n","Val loss: 0.2865256369113922 | Val R2: -0.8879986515743217 | Val MSE: 0.19404897093772888 | Val RMSE: 0.4405098855495453\n","----------\n","Epoch 102 / 125\n","----------\n","Train loss: 0.7682819962501526 | Train R2: -3.8658740448972275 | Train MSE: 0.6928692009920875 | Train RMSE: 0.7487288307812479\n","Val loss: 0.2865230143070221 | Val R2: -0.8879741667356842 | Val MSE: 0.19404643774032593 | Val RMSE: 0.44050702452659607\n","----------\n","Epoch 103 / 125\n","----------\n","Train loss: 0.7682815194129944 | Train R2: -3.865780979502755 | Train MSE: 0.6928692559401194 | Train RMSE: 0.7487279623746872\n","Val loss: 0.286520391702652 | Val R2: -0.8879500871825314 | Val MSE: 0.19404396414756775 | Val RMSE: 0.4405042231082916\n","----------\n","Epoch 104 / 125\n","----------\n","Train loss: 0.7682810425758362 | Train R2: -3.8656859298397475 | Train MSE: 0.6928693673883876 | Train RMSE: 0.7487271047300763\n","Val loss: 0.2865177392959595 | Val R2: -0.8879256176725918 | Val MSE: 0.194041445851326 | Val RMSE: 0.4405013620853424\n","----------\n","Epoch 105 / 125\n","----------\n","Train loss: 0.7682805061340332 | Train R2: -3.865592126873684 | Train MSE: 0.6928694213016166 | Train RMSE: 0.7487262181109853\n","Val loss: 0.2865149974822998 | Val R2: -0.8879009158724713 | Val MSE: 0.19403894245624542 | Val RMSE: 0.4404985010623932\n","----------\n","Epoch 106 / 125\n","----------\n","Train loss: 0.768280029296875 | Train R2: -3.865496495267763 | Train MSE: 0.6928695638974508 | Train RMSE: 0.748725353843636\n","Val loss: 0.2865122854709625 | Val R2: -0.8878754573028451 | Val MSE: 0.1940363198518753 | Val RMSE: 0.4404955506324768\n","----------\n","Epoch 107 / 125\n","----------\n","Train loss: 0.768279492855072 | Train R2: -3.8654024543317496 | Train MSE: 0.6928696117053429 | Train RMSE: 0.7487244697080718\n","Val loss: 0.28650957345962524 | Val R2: -0.8878505072042486 | Val MSE: 0.19403375685214996 | Val RMSE: 0.4404926300048828\n","----------\n","Epoch 108 / 125\n","----------\n","Train loss: 0.7682790160179138 | Train R2: -3.8653055852051605 | Train MSE: 0.6928697526454926 | Train RMSE: 0.748723603785038\n","Val loss: 0.2865068018436432 | Val R2: -0.8878248929879309 | Val MSE: 0.19403110444545746 | Val RMSE: 0.44048961997032166\n","----------\n","Epoch 109 / 125\n","----------\n","Train loss: 0.7682784795761108 | Train R2: -3.8652041218138016 | Train MSE: 0.6928698051099976 | Train RMSE: 0.748722648455037\n","Val loss: 0.2865040600299835 | Val R2: -0.8877993919057092 | Val MSE: 0.19402845203876495 | Val RMSE: 0.4404866099357605\n","----------\n","Epoch 110 / 125\n","----------\n","Train loss: 0.7682781219482422 | Train R2: -3.8651070939293013 | Train MSE: 0.6928699134538571 | Train RMSE: 0.7487217659751574\n","Val loss: 0.2865011692047119 | Val R2: -0.8877731691665165 | Val MSE: 0.19402578473091125 | Val RMSE: 0.44048357009887695\n","----------\n","Epoch 111 / 125\n","----------\n","Train loss: 0.7682774662971497 | Train R2: -3.8650092741488575 | Train MSE: 0.6928699788533978 | Train RMSE: 0.7487208627992206\n","Val loss: 0.2864983081817627 | Val R2: -0.887747188401768 | Val MSE: 0.19402311742305756 | Val RMSE: 0.4404805600643158\n","----------\n","Epoch 112 / 125\n","----------\n","Train loss: 0.7682769298553467 | Train R2: -3.864907579838567 | Train MSE: 0.6928700388719639 | Train RMSE: 0.7487199190590117\n","Val loss: 0.2864954471588135 | Val R2: -0.8877207036588841 | Val MSE: 0.19402040541172028 | Val RMSE: 0.4404774606227875\n","----------\n","Epoch 113 / 125\n","----------\n","Train loss: 0.768276572227478 | Train R2: -3.864803987032383 | Train MSE: 0.6928701632552676 | Train RMSE: 0.7487189927034907\n","Val loss: 0.28649258613586426 | Val R2: -0.8876943918735489 | Val MSE: 0.194017693400383 | Val RMSE: 0.44047439098358154\n","----------\n","Epoch 114 / 125\n","----------\n","Train loss: 0.7682759165763855 | Train R2: -3.8647040822361194 | Train MSE: 0.69287024003764 | Train RMSE: 0.7487180406848589\n","Val loss: 0.28648966550827026 | Val R2: -0.8876672292133401 | Val MSE: 0.19401492178440094 | Val RMSE: 0.44047126173973083\n","----------\n","Epoch 115 / 125\n","----------\n","Train loss: 0.7682754397392273 | Train R2: -3.8646021583217567 | Train MSE: 0.6928702849480841 | Train RMSE: 0.748717091149754\n","Val loss: 0.28648674488067627 | Val R2: -0.8876401957144007 | Val MSE: 0.1940121352672577 | Val RMSE: 0.44046807289123535\n","----------\n","Epoch 116 / 125\n","----------\n","Train loss: 0.7682750225067139 | Train R2: -3.8644998315606847 | Train MSE: 0.6928704091244273 | Train RMSE: 0.74871616727776\n","Val loss: 0.2864837348461151 | Val R2: -0.8876124626831621 | Val MSE: 0.19400927424430847 | Val RMSE: 0.4404648244380951\n","----------\n","Epoch 117 / 125\n","----------\n","Train loss: 0.7682743072509766 | Train R2: -3.864393536515407 | Train MSE: 0.6928704791805811 | Train RMSE: 0.7487151871124903\n","Val loss: 0.28648072481155396 | Val R2: -0.8875846446078548 | Val MSE: 0.19400641322135925 | Val RMSE: 0.4404616057872772\n","----------\n","Epoch 118 / 125\n","----------\n","Train loss: 0.7682738900184631 | Train R2: -3.86428780564295 | Train MSE: 0.6928705575151576 | Train RMSE: 0.7487141953574287\n","Val loss: 0.2864777445793152 | Val R2: -0.8875569592994588 | Val MSE: 0.19400358200073242 | Val RMSE: 0.44045838713645935\n","----------\n","Epoch 119 / 125\n","----------\n","Train loss: 0.7682731747627258 | Train R2: -3.864181059544462 | Train MSE: 0.6928706514752574 | Train RMSE: 0.7487132259541087\n","Val loss: 0.28647467494010925 | Val R2: -0.8875287850207041 | Val MSE: 0.19400069117546082 | Val RMSE: 0.4404551088809967\n","----------\n","Epoch 120 / 125\n","----------\n","Train loss: 0.7682726979255676 | Train R2: -3.864075177219119 | Train MSE: 0.6928707768933641 | Train RMSE: 0.7487122582064735\n","Val loss: 0.2864716649055481 | Val R2: -0.8875010423240797 | Val MSE: 0.1939978301525116 | Val RMSE: 0.44045186042785645\n","----------\n","Epoch 121 / 125\n","----------\n","Train loss: 0.7682721614837646 | Train R2: -3.8639626396707083 | Train MSE: 0.6928708237699337 | Train RMSE: 0.7487112101581361\n","Val loss: 0.286468505859375 | Val R2: -0.8874719918743246 | Val MSE: 0.19399482011795044 | Val RMSE: 0.44044843316078186\n","----------\n","Epoch 122 / 125\n","----------\n","Train loss: 0.7682716846466064 | Train R2: -3.8638564180117214 | Train MSE: 0.6928709334590368 | Train RMSE: 0.7487102266814973\n","Val loss: 0.2864654064178467 | Val R2: -0.8874428294608911 | Val MSE: 0.19399183988571167 | Val RMSE: 0.44044503569602966\n","----------\n","Epoch 123 / 125\n","----------\n","Train loss: 0.7682710886001587 | Train R2: -3.8637489998277212 | Train MSE: 0.6928710251425704 | Train RMSE: 0.7487092423770163\n","Val loss: 0.28646227717399597 | Val R2: -0.8874141996253189 | Val MSE: 0.19398890435695648 | Val RMSE: 0.44044172763824463\n","----------\n","Epoch 124 / 125\n","----------\n","Train loss: 0.7682704329490662 | Train R2: -3.863635398859971 | Train MSE: 0.6928711347281933 | Train RMSE: 0.7487082075741556\n","Val loss: 0.2864590883255005 | Val R2: -0.8873849721781857 | Val MSE: 0.19398587942123413 | Val RMSE: 0.44043827056884766\n","----------\n","Epoch 125 / 125\n","----------\n","Train loss: 0.7682698965072632 | Train R2: -3.8635260239746554 | Train MSE: 0.6928711852265729 | Train RMSE: 0.7487071885002984\n","Val loss: 0.2864559292793274 | Val R2: -0.8873557158859169 | Val MSE: 0.19398288428783417 | Val RMSE: 0.44043487310409546\n","----------\n","Best loss: 0.15890392661094666 | Best R2: 0.3164895052165293 | Best MSE: 0.07025138288736343 | Best RMSE: 0.2650497853755951\n"]}],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","def train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs):\n","    best_loss = np.inf\n","    best_r2 = -np.inf\n","    best_mse = np.inf\n","    best_rmse = np.inf\n","    best_model = None\n","\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch + 1} / {epochs}')\n","        print('-' * 10)\n","\n","        train_loss, train_r2, train_mse, train_rmse = model.train_one_epoch(train_loader, criterion, optimizer, device)\n","        print(f'Train loss: {train_loss} | Train R2: {train_r2} | Train MSE: {train_mse} | Train RMSE: {train_rmse}')\n","\n","        val_loss, val_r2, val_mse, val_rmse = model.validate_one_epoch(val_loader, criterion, device)\n","        print(f'Val loss: {val_loss} | Val R2: {val_r2} | Val MSE: {val_mse} | Val RMSE: {val_rmse}')\n","        print('-' * 10)\n","\n","        scheduler.step(val_loss)\n","\n","        if val_loss < best_loss:\n","            best_loss = val_loss\n","            best_r2 = val_r2\n","            best_mse = val_mse\n","            best_rmse = val_rmse\n","            best_model = copy.deepcopy(model)\n","\n","    print(f'Best loss: {best_loss} | Best R2: {best_r2} | Best MSE: {best_mse} | Best RMSE: {best_rmse}')\n","    return best_model\n","\n","model = ViTBase16(1)\n","model.to(device)\n","\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n","\n","model = train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=125)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UR2XkM4FlEcK"},"outputs":[],"source":["def test(model, test_loader, criterion, device):\n","    test_loss, test_r2, test_mse, test_rmse = model.validate_one_epoch(test_loader, criterion, device)\n","    print(f'Test loss: {test_loss} | Test R2: {test_r2} | Test MSE: {test_mse} | Test RMSE: {test_rmse}')\n","\n","test(model, test_loader, criterion, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wOPH_rndon9H"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPzzo8c0YqLG1wxlI3Eo2lu","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.16"}},"nbformat":4,"nbformat_minor":0}
