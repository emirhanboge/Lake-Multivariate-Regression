{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filepath = '/content/drive/MyDrive/LakeRegression/340_Veri_toplam_temiz.xlsx'\n",
    "feats_df = pd.read_excel(features_filepath)\n",
    "selected_feature = 'Klorofil-a (Âµg/L)'\n",
    "feats_df = feats_df[['Date', 'Station', selected_feature, 'X', 'Y']]\n",
    "feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "feats_df[selected_feature] = scaler.fit_transform(feats_df[selected_feature].values.reshape(-1, 1))\n",
    "feats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "unique_dates = feats_df['Date'].unique()\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_val = []\n",
    "y_val = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "patch_size = 9\n",
    "for i in tqdm(range(1, 35)):\n",
    "    if i != 22 and i != 23:\n",
    "        folder_path = f'/content/drive/MyDrive/LakeRegression/data/{i}'\n",
    "        for j in range(10):\n",
    "            file_path = f'{folder_path}/station_{j}_{patch_size}.h5'\n",
    "            if os.path.isfile(file_path):\n",
    "                with h5py.File(file_path, 'r') as f:\n",
    "                    a_group_key = list(f.keys())[0]\n",
    "                    image = np.array(f[a_group_key])\n",
    "\n",
    "                    unique_date = unique_dates[i - 1]\n",
    "                    unique_station = j + 1\n",
    "                    label = feats_df.loc[(feats_df['Date'] == unique_date) & (feats_df['Station'] == unique_station)][selected_feature].values[0]\n",
    "\n",
    "                    date = pd.to_datetime(unique_date)\n",
    "                    if date < pd.to_datetime('2019-03-15'):\n",
    "                        X_train.append(image)\n",
    "                        y_train.append(label)\n",
    "                    elif date >= pd.to_datetime('2019-03-15') and date < pd.to_datetime('2019-05-01'):\n",
    "                        X_val.append(image)\n",
    "                        y_val.append(label)\n",
    "                    else:\n",
    "                        X_test.append(image)\n",
    "                        y_test.append(label)\n",
    "\n",
    "print(f'Train % {len(X_train) / (len(X_train) + len(X_val) + len(X_test))} | Val % {len(X_val) / (len(X_train) + len(X_val) + len(X_test))} | Test % {len(X_test) / (len(X_train) + len(X_val) + len(X_test))}')\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f'Train shape: {X_train.shape} | {y_train.shape} | Val shape: {X_val.shape} | {y_val.shape} | Test shape: {X_test.shape} | {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class LakeDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "for i in range(12):\n",
    "    means.append(np.mean(X_train[:, i, :, :]))\n",
    "    stds.append(np.std(X_train[:, i, :, :]))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: torch.from_numpy(x).float()),\n",
    "    transforms.Normalize(means, stds)\n",
    "])\n",
    "\n",
    "train_dataset = LakeDataset(X_train, y_train, transform=transform)\n",
    "val_dataset = LakeDataset(X_val, y_val, transform=transform)\n",
    "test_dataset = LakeDataset(X_test, y_test, transform=transform)\n",
    "\n",
    "# use sequential sampler to preserve the date order\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=SequentialSampler(train_dataset))\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, sampler=SequentialSampler(val_dataset))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, sampler=SequentialSampler(test_dataset))\n",
    "\n",
    "print(f'Train loader: {len(train_loader)} | Val loader: {len(val_loader)} | Test loader: {len(test_loader)}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(inputs):\n",
    "    squared_norm = (inputs ** 2).sum(-1, keepdim=True)\n",
    "    output = squared_norm * inputs / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "    return output\n",
    "\n",
    "class PrimaryCapsules(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=12, out_channels=32, kernel_size=9, num_routes=32 * 6 * 6):\n",
    "        super(PrimaryCapsules, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0)\n",
    "            for _ in range(num_capsules)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        u = u.view(x.size(0), 32 * 6 * 6, -1)\n",
    "        return squash(u)\n",
    "\n",
    "\n",
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=1, num_routes=32 * 6 * 6, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "        if torch.cuda.is_available():\n",
    "            b_ij = b_ij.cuda()\n",
    "\n",
    "        num_iterations = 3\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij, dim=1)\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = squash(s_j)\n",
    "\n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "\n",
    "\n",
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(in_channels=12, out_channels=256, kernel_size=9, stride=1)\n",
    "        self.primary_capsules = PrimaryCapsules()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = nn.Linear(16 * 1, 1) \n",
    "\n",
    "    def forward(self, data):\n",
    "        output = self.conv_layer(data)\n",
    "        output = self.primary_capsules(output)\n",
    "        output = self.digit_capsules(output)\n",
    "        predictions = self.decoder(output)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, criterion, optimizer, epochs, device): \n",
    "    print(f'Using device: {device}')\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_actuals = []\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X)\n",
    "            loss = criterion(y_hat, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(y_hat.cpu().detach().numpy())\n",
    "            train_actuals.extend(y.cpu().numpy())\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        train_r2 = r2_score(train_actuals, train_predictions)\n",
    "        train_mse = mean_squared_error(train_actuals, train_predictions)\n",
    "        train_rmse = mean_squared_error(train_actuals, train_predictions, squared=False)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_actuals = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X, y in val_loader:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                y_hat = model(X)\n",
    "                loss = criterion(y_hat, y.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(y_hat.cpu().detach().numpy())\n",
    "                val_actuals.extend(y.cpu().numpy())\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        val_r2 = r2_score(val_actuals, val_predictions)\n",
    "        val_mse = mean_squared_error(val_actuals, val_predictions)\n",
    "        val_rmse = mean_squared_error(val_actuals, val_predictions, squared=False)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs} | Train loss: {train_loss:.4f} R2: {train_r2:.4f} MSE: {train_mse:.4f} RMSE: {train_rmse:.4f} | Val loss: {val_loss:.4f} R2: {val_r2:.4f} MSE: {val_mse:.4f} RMSE: {val_rmse:.4f}')\n",
    "   \n",
    "    return model\n",
    "\n",
    "capsule_net = CapsNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = optim.Adam(capsule_net.parameters())\n",
    "\n",
    "epochs = 50"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
